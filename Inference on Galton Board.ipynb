{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on the generalized Galton board\n",
    "\n",
    "This notebook uses the augmented data mined from the simulators to demonstrate the new simulation-based inference methods. The table below summarizes which methods utilize the joint score and joint ratio via the loss functions `L_t` and `L_r` .\n",
    "\n",
    "<img  src=\"figures/table.png\"  width=\"50%\" align=\"center\" />\n",
    "\n",
    "\n",
    "<!--\n",
    "<div>\n",
    "<img  src=\"figures/mining_ppl.png\"  width=\"200\" align=\"left\" />\n",
    "<img  src=\"figures/joint_ratio_eqtn.png\"  width=\"200\" align=\"left\" />\n",
    "<img  src=\"figures/joint_score_eqtn.png\"  width=\"200\" align=\"right\" />\n",
    "</div>\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "import import_ipynb\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, dot, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from galton.ipynb\n"
     ]
    }
   ],
   "source": [
    "from galton import galton_rvs, galton_rvs_ratio, check_random_state\n",
    "from galton import n_nails, n_rows\n",
    "from utils import calculate_height, adjust_margins\n",
    "\n",
    "theta_0 = -0.8\n",
    "theta_1 = -0.6\n",
    "\n",
    "n_features = 1 \n",
    "hidden_size = 10\n",
    "n_outputs = n_nails\n",
    "batch_size = 256\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"True densities\" from histograms\n",
    "\n",
    "The probability mass function `p(x|θ)` implicitly defined by this simulator is intractable. To estimate the true density we run the simulator many times and make histograms for the outcomes. These are estimates of the implicit distributions, but inefficient for inference in more complex real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_0, _, scores_0, _ = galton_rvs(theta_0, n_runs=20000, random_state=1234)\n",
    "p_estimated_0, _ = np.histogram(samples_0, bins=n_nails, range=(0, n_nails), density=True)\n",
    "\n",
    "samples_1, _, scores_1, _ = galton_rvs(theta_1, n_runs=20000, random_state=1234)\n",
    "p_estimated_1, _ = np.histogram(samples_1, bins=n_nails, range=(0, n_nails), density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(len(p_estimated_0)), p_estimated_0,\n",
    "         c='r', lw=1.5,\n",
    "         label=r'$p(x|\\theta_0)$')\n",
    "plt.step(range(len(p_estimated_0)), p_estimated_1,\n",
    "         c='b', lw=1.5, \n",
    "         label=r'$p(x|\\theta_1)$')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDE and SCANDAL\n",
    "\n",
    "First, we estimate the densities `p(x|θ)` (which are actually probability mass functions, since `x` is discrete) through a simple neural density estimator (NDE). We then compare this to the new SCANDAL (Score and neural density approximate likelihood) method. This technique uses the \"gold\" mined from the simulator, the joint score `t(x,z|θ)`, to guide the training of the neural density estimator. More precisely, the SCANDAL loss consists not just of the cross-entropy, but adds the mean squared error between the derived score and the joint score available from the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(n_samples, n_thetas=10, random_state=0):\n",
    "    n_traces_per_theta = n_samples // n_thetas\n",
    "    \n",
    "    all_x = []\n",
    "    all_t_xz = []\n",
    "    all_thetas = []\n",
    "    \n",
    "    for k, theta in enumerate(np.linspace(-1.0, -0.4, n_thetas)):\n",
    "        x, log_p_xz, t_xz, _ = galton_rvs(theta, n_runs=n_traces_per_theta, random_state=random_state+k)\n",
    "        all_x.append(x)\n",
    "        all_t_xz.append(t_xz)\n",
    "        all_thetas.append(theta * np.ones(n_traces_per_theta))\n",
    "        \n",
    "    all_x = np.array(all_x).reshape(-1, 1)\n",
    "    all_x = to_categorical(all_x, num_classes=n_nails)\n",
    "    all_t_xz = np.array(all_t_xz).reshape(-1, 1)\n",
    "    all_thetas = np.array(all_thetas).reshape(-1, 1)\n",
    "    \n",
    "    rng = check_random_state(random_state)\n",
    "    choices = rng.choice(len(all_x), len(all_x), replace=False)\n",
    "        \n",
    "    return all_x[choices], all_t_xz[choices], all_thetas[choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nde(lr=0.001):   \n",
    "    theta = Input(shape=(1,))\n",
    "    h = Dense(hidden_size, activation=\"tanh\")(theta)\n",
    "    phat_theta = Dense(n_outputs, activation=\"softmax\")(h)\n",
    "    \n",
    "    model = Model(inputs=[theta], outputs=[phat_theta])\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(loss=[\"categorical_crossentropy\"], optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scandal(lr=0.001):\n",
    "    theta = Input(shape=(1,))\n",
    "    h = Dense(hidden_size, activation=\"tanh\")(theta)\n",
    "    phat_theta = Dense(n_outputs, activation=\"softmax\")(h)\n",
    "    \n",
    "    x = Input(shape=(n_outputs,))\n",
    "    phat_x_theta = dot([x, phat_theta], axes=1) \n",
    "    t_x = Lambda(lambda exp: K.gradients(K.log(exp[0]), [exp[1]])[0], \n",
    "                 output_shape=(n_features,))([phat_x_theta, theta])\n",
    "    \n",
    "    model = Model(inputs=[theta, x], outputs=[phat_theta, t_x])\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(loss=[\"categorical_crossentropy\", \"mse\"], optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t_xz, theta = draw(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nde = make_nde()\n",
    "nde.fit(\n",
    "    theta, x,\n",
    "    batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scandal = make_scandal()\n",
    "scandal.fit(\n",
    "    [theta, x],\n",
    "    [x, t_xz],\n",
    "    batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(n_nails), p_estimated_0, label=\"histogram\")\n",
    "plt.step(range(n_nails), nde.predict(np.array([theta_0])).ravel(), label=\"nde\")\n",
    "plt.step(range(n_nails), scandal.predict([np.array([[theta_0]]), np.zeros((1, n_nails))])[0][0], label=\"scandal\")\n",
    "plt.legend()\n",
    "plt.title(r\"$\\theta=%.2f$\" % theta_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(n_nails), p_estimated_1, label=\"histogram\")\n",
    "plt.step(range(n_nails), nde.predict(np.array([theta_1])).ravel(), label=\"nde\")\n",
    "plt.step(range(n_nails), scandal.predict([np.array([[theta_1]]), np.zeros((1, n_nails))])[0][0], label=\"scandal\")\n",
    "plt.legend()\n",
    "plt.title(r\"$\\theta=%.2f$\" % theta_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood ratio trick (LRT)\n",
    "\n",
    "A discriminative classifier trained to distinguish between equal-sized samples `x~p(x|θ0)` and `x~p(x|θ1)` can be used to define an estimator for the likelihood ratio `r(x|θ0,θ1) = p(x|θ0) / p(x|θ1)`. The reason behind this \"likelihood ratio trick\" is that the binary cross-entropy between these two samples is minimized by the optimal decision function `s*(x|θ0,θ1) = p(x|θ1) / (p(x|θ0) + p(x|θ1))`, or `r(x|θ0,θ1) = (1 - s*(x|θ0,θ1)) / s*(x|θ0,θ1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lrt(n_samples, n_thetas=10, random_state=0):\n",
    "    grid = list(product(np.linspace(-1.0, -0.4, n_thetas), \n",
    "                        np.linspace(-1.0, -0.4, n_thetas)))\n",
    "    n_traces_per_theta = n_samples // (2 * len(grid))\n",
    "    all_x = []\n",
    "    all_theta = []\n",
    "    all_y = []\n",
    "    k = 0\n",
    "    for theta_0, theta_1 in grid:\n",
    "        x_0, _, _, _ = galton_rvs(theta_0, n_runs=n_traces_per_theta, random_state=random_state+k)\n",
    "        x_1, _, _, _ = galton_rvs(theta_1, n_runs=n_traces_per_theta, random_state=random_state+k)\n",
    "        all_x += x_0\n",
    "        all_x += x_1\n",
    "        all_theta += [[theta_0, theta_1]] * (n_traces_per_theta * 2)\n",
    "        all_y += [0] * len(x_0)\n",
    "        all_y += [1] * len(x_1)\n",
    "        k += 1\n",
    "    all_x = np.array(all_x).reshape(-1, 1)\n",
    "    all_x = to_categorical(all_x, num_classes=n_nails)\n",
    "    all_theta = np.array(all_theta)\n",
    "    all_x = np.hstack([all_x, all_theta])\n",
    "    all_y = np.array(all_y)\n",
    "    rng = check_random_state(random_state)\n",
    "    choices = rng.choice(len(all_x), len(all_x), replace=False)\n",
    "        \n",
    "    return all_x[choices], all_y[choices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lrt(lr=0.001):\n",
    "    inputs = Input(shape=(2 + n_outputs, )) \n",
    "    thetas = Lambda(lambda x: x[:,-2:], output_shape=(2,))(inputs)\n",
    "    x = Lambda(lambda x: x[:,:-2], output_shape=(n_outputs,))(inputs)\n",
    "    h = Dense(hidden_size, activation=\"tanh\")(inputs)\n",
    "    shat_thetas = Dense(n_outputs, activation=\"sigmoid\")(h)\n",
    "\n",
    "    shat_x_thetas = dot([x, shat_thetas], axes=1)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[shat_x_thetas])\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(loss=[\"binary_crossentropy\"], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_r_from_s(s, epsilon=1.e-6):\n",
    "    return np.log((1. - s + epsilon) / (s + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = draw_lrt(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrt = make_lrt()\n",
    "\n",
    "lrt.fit(\n",
    "    x, y, \n",
    "    batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_true = np.log(p_estimated_0[5:-5]) - np.log(p_estimated_1[5:-5])\n",
    "\n",
    "x_test = to_categorical(np.array(range(n_nails)), num_classes=n_nails)\n",
    "r_lrt = log_r_from_s(lrt.predict(np.hstack([x_test,\n",
    "                                 theta_0 * np.ones((n_nails, 1)), \n",
    "                                 theta_1 * np.ones((n_nails, 1))]))[5:-5])\n",
    "\n",
    "\n",
    "plt.step(range(5, n_nails-5), r_true, label=\"Truth\")\n",
    "plt.step(range(5, n_nails-5), r_lrt, label=\"LRT\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROLR and RASCAL\n",
    "\n",
    "The \"gold\" that can be minded from the simulator consists not only of the score information used in SCANDAL, but also the joint likelihood ratios `r(x,z|θ0,θ1) = p(x,z|θ0) / p(x,z|θ1)`. The squared error loss functional between a function `r̂(x)` and the joint likelihood ratio available from the simulator sampled according to `θ1` is minimized by the intractable likelihood ratio `r̂(x) = r(x|θ0,θ1) = p(x|θ0) / p(x|θ1)`! Conversely, the squared error on `1/r(x|θ0,θ1)`, sampled according to `θ1`, is minimized by `1/r(x|θ0,θ1)`. This is the foundation of the ROLR (Regression on likelihood ratio) technique.\n",
    "\n",
    "In the same spirit as NDE augmented with the score information from the simulator defined the SCANDAL technique, we can combine the ROLR method with the score information to define the RASCAL (Ratio and score approximate likelihood ratio) method. Here we minimize a loss function that combines the squared error on the joint likelihood ratio with the squared error between the derived estimated score and the joint score available from the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ratio(n_samples, n_thetas=10, random_state=0):\n",
    "    grid = list(product(np.linspace(-1.0, -0.4, n_thetas), \n",
    "                        np.linspace(-1.0, -0.4, n_thetas)))\n",
    "    n_traces_per_theta = n_samples // len(grid)\n",
    "    \n",
    "    rng = check_random_state(random_state)\n",
    "    \n",
    "    all_x = []\n",
    "    all_log_r_xz = []\n",
    "    all_t_xz_0 = []\n",
    "    all_thetas = []\n",
    "\n",
    "    # draw from theta_0\n",
    "    for theta_0, theta_1 in grid:\n",
    "        x, log_p_xz_0, log_p_xz_1, t_xz_0, _, _ = galton_rvs_ratio(theta_0, theta_1, \n",
    "                                                                   n_runs=n_traces_per_theta, random_state=rng)\n",
    "        all_x.append(x)\n",
    "        all_log_r_xz.append(log_p_xz_0 - log_p_xz_1)\n",
    "        all_t_xz_0.append(t_xz_0)\n",
    "        all_thetas.append(np.tile(np.array([theta_0, theta_1]), (n_traces_per_theta, 1)))\n",
    "        \n",
    "    # reshape\n",
    "    all_x = np.array(all_x).reshape(-1, 1)\n",
    "    all_x = to_categorical(all_x, num_classes=n_nails)\n",
    "    all_log_r_xz = np.array(all_log_r_xz).reshape(-1, 1)\n",
    "    all_t_xz_0 = np.array(all_t_xz_0).reshape(-1, 1)\n",
    "    all_thetas = np.vstack(all_thetas)\n",
    "    \n",
    "    choices = rng.choice(len(all_x), len(all_x), replace=False)\n",
    "        \n",
    "    return all_x[choices], all_thetas[choices], all_log_r_xz[choices], all_t_xz_0[choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rolr(lr=0.001):\n",
    "    theta0 = Input(shape=(1,))\n",
    "    theta1 = Input(shape=(1,))\n",
    "    thetas = concatenate([theta0, theta1])\n",
    "    \n",
    "    h = Dense(hidden_size, activation=\"tanh\")(thetas)\n",
    "    log_rhat_thetas = Dense(n_outputs, activation=\"linear\")(h)\n",
    "    rhat_thetas = Lambda(lambda exp: K.exp(exp))(log_rhat_thetas)\n",
    "    \n",
    "    x = Input(shape=(n_outputs,))\n",
    "    rhat_x_thetas = dot([x, rhat_thetas], axes=1)\n",
    "    \n",
    "    model = Model(inputs=[theta0, theta1, x], outputs=[rhat_x_thetas])\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(loss=[inv_mse], optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def inv_mse(y_true, y_pred):\n",
    "    inverse_r_loss = mean_squared_error((1./ K.clip(y_true, -10., 10.)),\n",
    "                                               (1./ K.clip(y_pred, -10., 10.)))\n",
    "\n",
    "    return inverse_r_loss\n",
    "\n",
    "\n",
    "def make_rascal(lr=0.001):\n",
    "    theta0 = Input(shape=(1,))\n",
    "    theta1 = Input(shape=(1,))\n",
    "    thetas = concatenate([theta0, theta1])\n",
    "    \n",
    "    h = Dense(hidden_size, activation=\"tanh\")(thetas)\n",
    "    log_rhat_thetas = Dense(n_outputs, activation=\"linear\")(h)\n",
    "    rhat_thetas = Lambda(lambda exp: K.exp(exp))(log_rhat_thetas)\n",
    "    \n",
    "    x = Input(shape=(n_outputs,))\n",
    "    rhat_x_thetas = dot([x, rhat_thetas], axes=1)\n",
    "    \n",
    "    log_rhat_x_theta = dot([x, log_rhat_thetas], axes=1) \n",
    "    t_x_0 = Lambda(lambda exp: K.gradients(exp[0], [exp[1]])[0], \n",
    "                   output_shape=(n_features,))([log_rhat_x_theta, theta0])\n",
    "    \n",
    "    model = Model(inputs=[theta0, theta1, x], outputs=[rhat_x_thetas, t_x_0])\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(loss=[inv_mse, \"mse\"], loss_weights=[1.0, 0.5], optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, thetas, log_r_xz, t_xz_0 = draw_ratio(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rolr = make_rolr()\n",
    "rolr.fit(\n",
    "    [thetas[:, 0], thetas[:, 1], x], \n",
    "    np.exp(log_r_xz),\n",
    "    batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rascal = make_rascal()\n",
    "rascal.fit(\n",
    "    [thetas[:, 0], thetas[:, 1], x], \n",
    "    [np.exp(log_r_xz), t_xz_0], \n",
    "    batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_true = np.exp(np.log(p_estimated_0[5:-5]) - np.log(p_estimated_1[5:-5]))\n",
    "\n",
    "x_test = to_categorical(np.array(range(n_nails)), num_classes=n_nails)\n",
    "r_rolr = rolr.predict([theta_0 * np.ones(n_nails), \n",
    "                       theta_1 * np.ones(n_nails),\n",
    "                       x_test])[5:-5]\n",
    "r_rascal = rascal.predict([theta_0 * np.ones(n_nails),\n",
    "                           theta_1 * np.ones(n_nails),\n",
    "                           x_test])[0][5:-5]\n",
    "\n",
    "plt.step(range(5, n_nails-5), r_true, label=\"estimated\")\n",
    "plt.step(range(5, n_nails-5), r_rolr, label=\"rolr\")\n",
    "plt.step(range(5, n_nails-5), r_rascal, label=\"rascal\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "We now compare how well the different inference methods let us estimate the likelihood ratio `r(x|θ0,θ1) = p(x|θ0) / p(x|θ1)` as a function of the training sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(p_train, p_estimated):\n",
    "    p_ratio_train =  np.log(p_train[0]) - np.log(p_train[1])\n",
    "    p_ratio = np.log(p_estimated[0]) - np.log(p_estimated[1])\n",
    "    mse = p_ratio_train - p_ratio\n",
    "    mse[np.logical_or(np.isinf(mse),np.isnan(mse))] = 0.\n",
    "    mse = np.dot(mse**2, p_estimated[1])\n",
    "    return mse\n",
    "\n",
    "\n",
    "def compute_mse_ratio(ratio, p_estimated, log=True):\n",
    "    p_ratio = np.log(p_estimated[0]) - np.log(p_estimated[1])\n",
    "    if log:\n",
    "        p_ratio_train = np.log(ratio)\n",
    "    else:\n",
    "        p_ratio_train = ratio\n",
    "    mse = p_ratio_train - p_ratio\n",
    "    mse[np.logical_or(np.isinf(mse),np.isnan(mse))] = 0.\n",
    "    mse = np.dot(mse**2, p_estimated[1])\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses_nde = []\n",
    "mses_scandal = []\n",
    "mses_lrt = []\n",
    "mses_rolr = []\n",
    "mses_rascal = []\n",
    "\n",
    "sample_sizes = [200,500,1000,2000,5000,10000]\n",
    "p_estimated = np.array([p_estimated_0, p_estimated_1])\n",
    "x_test = to_categorical(np.array(range(n_nails)), num_classes=n_nails)\n",
    "\n",
    "for k, n in enumerate(sample_sizes):\n",
    "    print('{0} Sample'.format(n))\n",
    "    \n",
    "    # NDE - SCANDAL\n",
    "    x, t_xz, theta = draw(n, random_state = 100 + k)\n",
    "    \n",
    "    nde = make_nde()\n",
    "    nde.fit(\n",
    "        theta,\n",
    "        x,\n",
    "        batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    "    )\n",
    "    \n",
    "    scandal = make_scandal()\n",
    "    scandal.fit(\n",
    "        [theta, x],\n",
    "        [x, t_xz],\n",
    "        batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    "    )\n",
    "    \n",
    "    nde_pred = nde.predict([theta_0, theta_1])\n",
    "    scandal_pred = scandal.predict([np.array([[theta_0], [theta_1]]), np.zeros((2, n_nails))])[0]\n",
    "    mse_nde = compute_mse(nde_pred, p_estimated)\n",
    "    mse_scandal = compute_mse(scandal_pred, p_estimated)\n",
    "    \n",
    "    # LRT\n",
    "    x, y = draw_lrt(n, random_state = 200 + k)\n",
    "    \n",
    "    lrt = make_lrt()\n",
    "    lrt.fit(\n",
    "        x, y, , \n",
    "        batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    "    )\n",
    "\n",
    "    lrt_pred = log_r_from_s(\n",
    "        lrt.predict(\n",
    "            np.hstack([x_test, theta_0 * np.ones((n_nails, 1)), theta_1 * np.ones((n_nails, 1))])\n",
    "        )[5:-5]\n",
    "    )\n",
    "    mse_lrt = compute_mse_ratio(lrt_pred.flatten(), p_estimated[:,5:-5], log=False)\n",
    "    \n",
    "    # ROLR - RASCAL\n",
    "    x, thetas, log_r_xz, t_xz_0 = draw_ratio(n, random_state = 300 + k)\n",
    "    \n",
    "    rolr = make_rolr()\n",
    "    rolr.fit(\n",
    "        [thetas[:, 0], thetas[:, 1], x], \n",
    "        np.exp(log_r_xz), \n",
    "        batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    "    )\n",
    "    \n",
    "    rascal = make_rascal()\n",
    "    rascal.fit(\n",
    "        [thetas[:, 0], thetas[:, 1], x], \n",
    "        [np.exp(log_r_xz), t_xz_0], \n",
    "        batch_size=batch_size, epochs=n_epochs, verbose=0\n",
    "    )\n",
    "    \n",
    "    rolr_pred = rolr.predict([theta_0 * np.ones(n_nails), \n",
    "                           theta_1 * np.ones(n_nails),\n",
    "                           x_test])[5:-5]\n",
    "    rascal_pred = rascal.predict([theta_0 * np.ones(n_nails),\n",
    "                               theta_1 * np.ones(n_nails),\n",
    "                               x_test])[0][5:-5]\n",
    "    mse_rolr = compute_mse_ratio(rolr_pred.flatten(), p_estimated[:, 5:-5], log=True)\n",
    "    mse_rascal = compute_mse_ratio(rascal_pred.flatten(), p_estimated[:, 5:-5], log=True)\n",
    "    \n",
    "    # Save results\n",
    "    mses_nde.append(mse_nde)\n",
    "    mses_rascal.append(mse_rascal)\n",
    "    mses_lrt.append(mse_lrt)\n",
    "    mses_rolr.append(mse_rolr)\n",
    "    mses_scandal.append(mse_scandal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses       = [mses_lrt, mses_nde, mses_rolr, mses_rascal, mses_scandal]\n",
    "labels     = ['LRT', 'NDE', 'ROLR', 'RASCAL', 'SCANDAL']\n",
    "colors     = ['darkgreen', 'slateblue',  'c', '#CC002E', '0.5']\n",
    "linestyles = [':', '--', ':', '-.', '--']\n",
    "linewidths = [1.5] * 5\n",
    "markers    = ['o'] * 5\n",
    "\n",
    "fig = plt.figure(figsize=(4.5,calculate_height(1,4.5,extra_top_space=False)))\n",
    "ax = plt.gca()\n",
    "\n",
    "for s, (mse, label) in enumerate(zip(mses, labels)):\n",
    "    plt.plot(sample_sizes, mse, \n",
    "             ms=4., marker=markers[s],\n",
    "             color=colors[s],\n",
    "             lw=linewidths[s], ls=linestyles[s])\n",
    "    plt.plot([], [],\n",
    "             color=colors[s],\n",
    "             lw=linewidths[s], ls=linestyles[s],\n",
    "             label=label)\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylim(0.00, 0.165)\n",
    "plt.xlabel(r\"Training sample size\")\n",
    "plt.ylabel(r'MSE$(\\log\\,r(x|\\theta_0, \\theta_1),\\log\\,\\hat{r}(x|\\theta_0, \\theta_1))$')\n",
    "\n",
    "adjust_margins(1,4.5,extra_top_space=False)\n",
    "plt.savefig('figures/model_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
